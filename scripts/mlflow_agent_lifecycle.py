"""
MLflow Agent Lifecycle Script

This script demonstrates the complete lifecycle of logging, registering, promoting,
and serving the customer service agent using MLflow following the ResponsesAgent pattern.

The script uses a template-based approach:
1. Reads the MLflow wrapper template from scripts/templates/
2. Combines it with the actual agent class from src/dev_agents/
3. Saves the combined file to scripts/logged_agents/ for MLflow logging

Based on: https://mlflow.org/docs/3.4.0/genai/serving/responses-agent/#logging-and-serving
"""

import os
from pathlib import Path

import mlflow
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set up paths
project_root = Path(__file__).parent.parent
scripts_dir = Path(__file__).parent
logged_agents_dir = scripts_dir / "logged_agents"
templates_dir = scripts_dir / "templates"


def create_agent_file_from_template(
    agent_source_file: Path,
    agent_class_name: str,
    output_name: str = "mlflow_agent"
) -> Path:
    """
    Create an MLflow-compatible agent file by combining the template wrapper
    with the actual agent class code.
    
    Args:
        agent_source_file: Path to the source agent file (e.g., customer_service_agent.py).
        agent_class_name: Name of the agent class to wrap (e.g., "CustomerServiceAgent").
        output_name: Name for the output file (without .py extension).
    
    Returns:
        Path to the created agent file.
    """
    # Read the template wrapper
    template_file = templates_dir / "mlflow_responses_agent_wrapper.py"
    with open(template_file, 'r') as f:
        wrapper_code = f.read()
    
    # Extract the relative import path for the agent
    # E.g., src/dev_agents/customer_service_agent.py -> dev_agents.customer_service_agent
    relative_to_src = agent_source_file.relative_to(project_root / "src")
    module_path = str(relative_to_src.with_suffix('')).replace(os.sep, '.')
    
    # Extract imports and class from template
    # We need to skip the path setup section but keep everything else
    lines = wrapper_code.split('\n')
    
    # Find the start of imports (after docstring)
    imports_start = 0
    for i, line in enumerate(lines):
        if line.startswith('import ') or line.startswith('from '):
            imports_start = i
            break
    
    # Find where the path setup section starts and ends
    path_setup_start = -1
    path_setup_end = -1
    for i, line in enumerate(lines):
        if '# Add the src directory to the path' in line:
            path_setup_start = i
        elif path_setup_start != -1 and (line.startswith('# Load environment') or line.startswith('load_dotenv')):
            path_setup_end = i + 1  # Include load_dotenv() line
            break
    
    # Extract template content, excluding the path setup section
    if path_setup_start != -1 and path_setup_end != -1:
        template_content = '\n'.join(lines[imports_start:path_setup_start] + lines[path_setup_end:])
    else:
        # Fallback: use everything from imports onward
        template_content = '\n'.join(lines[imports_start:])
    
    # Create the combined agent file
    combined_code = f'''"""
MLflow-compatible {agent_class_name} for serving.

This file is auto-generated by combining the template wrapper with the actual agent.
"""

import sys
from pathlib import Path

# Add the src directory to the path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

# Import the agent class
from {module_path} import {agent_class_name}

# Template code (imports and wrapper class)
{template_content}

# Create the specific agent instance using the wrapper
from mlflow.models import set_model

agent = MLflowResponsesAgentWrapper(
    agent_class={agent_class_name},
    model="gpt-4o"
)
set_model(agent)
'''
    
    # Ensure the logged_agents directory exists
    logged_agents_dir.mkdir(parents=True, exist_ok=True)
    
    # Save the combined file
    output_file = logged_agents_dir / f"{output_name}.py"
    with open(output_file, 'w') as f:
        f.write(combined_code)
    
    return output_file


def log_agent():
    """
    Log the customer service agent to MLflow using models-from-code approach.
    
    Returns:
        The logged model info.
    """
    print("\n" + "=" * 70)
    print("STEP 1: Logging Agent to MLflow")
    print("=" * 70)
    
    # Create agent file from template
    agent_source = project_root / "src" / "dev_agents" / "customer_service_agent.py"
    agent_file = create_agent_file_from_template(
        agent_source_file=agent_source,
        agent_class_name="CustomerServiceAgent",
        output_name="customer_service_mlflow_agent"
    )
    print(f"✓ Created agent file: {agent_file}")
    
    # Get the relative path from project root for MLflow
    relative_agent_path = agent_file.relative_to(project_root)
    
    # Print MLflow tracking URI
    print(f"✓ MLflow tracking URI: {mlflow.get_tracking_uri()}")
    
    # Create or set experiment
    experiment_name = "customer-service-agent"
    mlflow.set_experiment(experiment_name)
    print(f"✓ Using experiment: {experiment_name}")
    
    # Start a run and log the model
    with mlflow.start_run(run_name="customer_service_agent_v1") as run:
        print(f"✓ Started run: {run.info.run_id}")
        
        # Log the agent using models-from-code
        logged_agent_info = mlflow.pyfunc.log_model(
            python_model=str(relative_agent_path),
            artifact_path="agent",
            pip_requirements=[
                "mlflow>=3.4.0",
                "pydantic>=2.0.0",
                "openai-agents>=0.3.0",
                "faiss-cpu>=1.12.0",
                "sentence-transformers>=5.0.0",
                "numpy>=2.0.0",
                "python-dotenv>=1.0.0",
            ],
            metadata={"task": "customer_service", "version": "1.0.0"},
        )
        
        print(f"✓ Logged agent to: {logged_agent_info.model_uri}")
        
        # Log additional metadata
        mlflow.log_param("model_type", "gpt-4o")
        mlflow.log_param("agent_type", "customer_service")
        mlflow.log_param("num_tools", 5)
        
        print("✓ Logged parameters")
    
    return logged_agent_info


def register_model(model_uri: str, model_name: str = "customer-service-agent"):
    """
    Register the logged model in the MLflow Model Registry.
    
    Args:
        model_uri: The URI of the logged model.
        model_name: The name to register the model under.
    
    Returns:
        The registered model version.
    """
    print("\n" + "=" * 70)
    print("STEP 2: Registering Model in Model Registry")
    print("=" * 70)
    
    # Register the model
    model_version = mlflow.register_model(
        model_uri=model_uri,
        name=model_name,
        tags={"task": "customer_service", "framework": "openai-agents"}
    )
    
    print(f"✓ Registered model: {model_name}")
    print(f"✓ Version: {model_version.version}")
    print(f"✓ Current stage: {model_version.current_stage}")
    
    return model_version


def promote_to_production(model_name: str, version: int):
    """
    Promote the model version to production.
    
    Args:
        model_name: The name of the registered model.
        version: The version number to promote.
    """
    print("\n" + "=" * 70)
    print("STEP 3: Promoting Model to Production")
    print("=" * 70)
    
    from mlflow.tracking import MlflowClient
    
    client = MlflowClient()
    
    # Transition the model to production
    client.transition_model_version_stage(
        name=model_name,
        version=version,
        stage="Production",
        archive_existing_versions=True
    )
    
    print(f"✓ Promoted {model_name} version {version} to Production")
    print(f"✓ Previous production versions have been archived")


def load_production_model(model_name: str):
    """
    Load the production version of the model from MLflow.
    
    Args:
        model_name: The name of the registered model.
    
    Returns:
        The loaded model.
    """
    print("\n" + "=" * 70)
    print("STEP 4: Loading Production Model from MLflow")
    print("=" * 70)
    
    # Load the production model
    model_uri = f"models:/{model_name}/Production"
    print(f"Loading from: {model_uri}")
    
    loaded_model = mlflow.pyfunc.load_model(model_uri)
    print(f"✓ Successfully loaded production model")
    
    return loaded_model


def test_loaded_model(loaded_model):
    """
    Test the loaded model with a sample query.
    
    Args:
        loaded_model: The loaded MLflow model.
    """
    print("\n" + "=" * 70)
    print("STEP 5: Testing Loaded Model")
    print("=" * 70)
    
    # Test queries
    test_queries = [
        {
            "input": [{"role": "user", "content": "Can you check the status of order ORD12345?"}],
            "context": {"conversation_id": "test-001", "user_id": "test-user"}
        },
        {
            "input": [{"role": "user", "content": "How long does shipping take?"}],
            "context": {"conversation_id": "test-002", "user_id": "test-user"}
        }
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n--- Test Query {i} ---")
        print(f"Input: {query['input'][0]['content']}")
        
        try:
            result = loaded_model.predict(query)
            print(f"✓ Response received")
            print(f"Output: {result}")
        except Exception as e:
            print(f"✗ Error: {e}")
        
        print("-" * 70)


def interactive_chat(loaded_model):
    """
    Run an interactive chat session with the loaded model.
    
    Args:
        loaded_model: The loaded MLflow model.
    """
    print("\n" + "=" * 70)
    print("STEP 6: Interactive Chat with Loaded Model")
    print("=" * 70)
    print("Type your questions or requests below.")
    print("Type 'quit', 'exit', or 'bye' to end the conversation.")
    print("=" * 70 + "\n")
    
    conversation_id = "interactive-session"
    user_id = "demo-user"
    
    while True:
        try:
            # Get user input
            user_input = input("You: ").strip()
            
            # Check for exit commands
            if user_input.lower() in ['quit', 'exit', 'bye']:
                print("\nThank you for using the customer service agent. Goodbye!")
                break
            
            # Skip empty input
            if not user_input:
                continue
            
            # Create the request
            request = {
                "input": [{"role": "user", "content": user_input}],
                "context": {"conversation_id": conversation_id, "user_id": user_id}
            }
            
            # Get the response from the model
            result = loaded_model.predict(request)
            
            # Extract and display the response
            # The result is a ResponsesAgentResponse
            if isinstance(result, dict) and "output" in result:
                # Extract text from the output
                output_items = result["output"]
                for item in output_items:
                    if item.get("type") == "message":
                        for content in item.get("content", []):
                            if content.get("type") == "output_text":
                                print(f"\nAgent: {content.get('text')}\n")
            else:
                print(f"\nAgent: {result}\n")
            
        except KeyboardInterrupt:
            print("\n\nInterrupted. Thank you for using the customer service agent. Goodbye!")
            break
        except Exception as e:
            print(f"\nError: {e}\n")
            continue


def main():
    """
    Main function that orchestrates the complete MLflow agent lifecycle.
    """
    print("\n" + "=" * 70)
    print("MLflow Agent Lifecycle Demo")
    print("Customer Service Agent - Log, Register, Promote, Load, Chat")
    print("=" * 70)
    
    # Check for OpenAI API key
    if not os.getenv("OPENAI_API_KEY"):
        print("\n⚠️  Warning: OPENAI_API_KEY not found in environment variables.")
        print("Please set your OpenAI API key in a .env file or environment variable.")
        return
    
    try:
        # Step 1: Log the agent
        logged_info = log_agent()
        
        # Step 2: Register the model
        model_name = "customer-service-agent"
        model_version = register_model(logged_info.model_uri, model_name)
        
        # Step 3: Promote to production
        promote_to_production(model_name, model_version.version)
        
        # Step 4: Load the production model
        loaded_model = load_production_model(model_name)
        
        # Step 5: Test the loaded model
        test_loaded_model(loaded_model)
        
        # Step 6: Interactive chat
        print("\nWould you like to start an interactive chat? (y/n): ", end="")
        response = input().strip().lower()
        if response == 'y':
            interactive_chat(loaded_model)
        
        print("\n" + "=" * 70)
        print("✓ MLflow Agent Lifecycle Demo Completed Successfully!")
        print("=" * 70)
        
    except Exception as e:
        print(f"\n✗ Error during lifecycle demo: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
